{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BINARIA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos (reemplaza 'nombre_del_archivo.csv' con el nombre de tu archivo de datos)\n",
    "data = pd.read_csv('nombre_del_archivo.csv')\n",
    "\n",
    "# Cargar los datos (reemplaza 'nombre_del_archivo.csv' con el nombre de tu archivo de datos)\n",
    "data = pd.read_csv('nombre_del_archivo.csv')\n",
    "\n",
    "# Paso 2: Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.drop('etiqueta', axis=1)\n",
    "y = data['etiqueta']\n",
    "\n",
    "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 4: Crear un pipeline con modelos para comparar y seleccionar\n",
    "models = [('Logistic Regression', LogisticRegression()),\n",
    "          ('Support Vector Machine', SVC(probability=True)),\n",
    "          ('Random Forest', RandomForestClassifier())]\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), (name, model)])\n",
    "    \n",
    "    # Ajuste de hiperparámetros utilizando GridSearchCV\n",
    "    if name == 'Logistic Regression':\n",
    "        param_grid = {'Logistic Regression__C': [0.1, 1, 10],\n",
    "                      'Logistic Regression__solver': ['lbfgs', 'liblinear']}\n",
    "    elif name == 'Support Vector Machine':\n",
    "        param_grid = {'Support Vector Machine__C': [0.1, 1, 10],\n",
    "                      'Support Vector Machine__kernel': ['linear', 'rbf']}\n",
    "    elif name == 'Random Forest':\n",
    "        param_grid = {'Random Forest__n_estimators': [50, 100, 200],\n",
    "                      'Random Forest__max_depth': [None, 10, 20]}\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Entrenar el modelo por minibatch (opcional)\n",
    "    if hasattr(model, 'partial_fit'):\n",
    "        batch_size = 32\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            best_model.partial_fit(X_batch, y_batch, classes=np.unique(y))\n",
    "    else:\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar el modelo en el conjunto de prueba\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "    \n",
    "    # Métricas de rendimiento\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "    results.append(roc_auc)\n",
    "    names.append(name)\n",
    "    \n",
    "    print(f\"Modelo: {name}\")\n",
    "    print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "# Paso 5: Comparar y seleccionar el mejor modelo\n",
    "best_model_name = names[np.argmax(results)]\n",
    "best_model = [model[1] for model in models if model[0] == best_model_name][0]\n",
    "\n",
    "print(f\"Mejor Modelo: {best_model_name}\")\n",
    "\n",
    "# Paso 6: Entrenar el mejor modelo en el conjunto de entrenamiento completo\n",
    "best_pipeline = Pipeline([('scaler', StandardScaler()), (best_model_name, best_model)])\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Paso 7: Evaluar el mejor modelo en el conjunto de prueba\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_pred_proba = best_pipeline.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(f\"Mejor Modelo: {best_model_name}\")\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_proba))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Paso 8: Gráficas para observar el rendimiento del mejor modelo\n",
    "fpr, tpr, _ = roc_curve(y_test, y_pred_proba)\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(fpr, tpr, label=f'{best_model_name} (area = {roc_auc_score(y_test, y_pred_proba):.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MULTICLASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, roc_curve\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cargar los datos (reemplaza 'nombre_del_archivo.csv' con el nombre de tu archivo de datos)\n",
    "data = pd.read_csv('nombre_del_archivo.csv')\n",
    "\n",
    "# Paso 2: Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.drop('etiqueta', axis=1)\n",
    "y = data['etiqueta']\n",
    "\n",
    "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 4: Crear un pipeline con modelos para comparar y seleccionar\n",
    "models = [('Logistic Regression', LogisticRegression(multi_class='ovr')),\n",
    "          ('Support Vector Machine', SVC(probability=True)),\n",
    "          ('Random Forest', RandomForestClassifier())]\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), (name, model)])\n",
    "    \n",
    "    # Ajuste de hiperparámetros utilizando GridSearchCV\n",
    "    if name == 'Logistic Regression':\n",
    "        param_grid = {'Logistic Regression__C': [0.1, 1, 10],\n",
    "                      'Logistic Regression__solver': ['lbfgs', 'liblinear']}\n",
    "    elif name == 'Support Vector Machine':\n",
    "        param_grid = {'Support Vector Machine__C': [0.1, 1, 10],\n",
    "                      'Support Vector Machine__kernel': ['linear', 'rbf']}\n",
    "    elif name == 'Random Forest':\n",
    "        param_grid = {'Random Forest__n_estimators': [50, 100, 200],\n",
    "                      'Random Forest__max_depth': [None, 10, 20]}\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Entrenar el modelo por minibatch (opcional)\n",
    "    if hasattr(model, 'partial_fit'):\n",
    "        batch_size = 32\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            best_model.partial_fit(X_batch, y_batch, classes=np.unique(y))\n",
    "    else:\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar el modelo en el conjunto de prueba\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    y_pred_proba = best_model.predict_proba(X_test)\n",
    "    \n",
    "    # Métricas de rendimiento\n",
    "    roc_auc = roc_auc_score(y_test, y_pred_proba, multi_class='ovr')\n",
    "    results.append(roc_auc)\n",
    "    names.append(name)\n",
    "    \n",
    "    print(f\"Modelo: {name}\")\n",
    "    print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "    print(\"ROC AUC:\", roc_auc)\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(confusion_matrix(y_test, y_pred))\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Paso 5: Comparar y seleccionar el mejor modelo\n",
    "best_model_name = names[np.argmax(results)]\n",
    "best_model = [model[1] for model in models if model[0] == best_model_name][0]\n",
    "\n",
    "print(f\"Mejor Modelo: {best_model_name}\")\n",
    "\n",
    "# Paso 6: Entrenar el mejor modelo en el conjunto de entrenamiento completo\n",
    "best_pipeline = Pipeline([('scaler', StandardScaler()), (best_model_name, best_model)])\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Paso 7: Evaluar el mejor modelo en el conjunto de prueba\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "y_pred_proba = best_pipeline.predict_proba(X_test)\n",
    "\n",
    "print(f\"Mejor Modelo: {best_model_name}\")\n",
    "print(\"ROC AUC:\", roc_auc_score(y_test, y_pred_proba, multi_class='ovr'))\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "# Paso 8: Gráficas para observar el rendimiento del mejor modelo\n",
    "# Las curvas ROC no son aplicables para clasificación multiclase, se puede usar la matriz de confusión\n",
    "import seaborn as sns\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "plt.xlabel('Predicciones')\n",
    "plt.ylabel('Valores reales')\n",
    "plt.title(f'Matriz de Confusión - {best_model_name}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regresión"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Importar las bibliotecas y cargar los datos\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Cargar los datos (reemplaza 'nombre_del_archivo.csv' con el nombre de tu archivo de datos)\n",
    "data = pd.read_csv('nombre_del_archivo.csv')\n",
    "\n",
    "# Paso 2: Dividir los datos en características (X) y etiquetas (y)\n",
    "X = data.drop('etiqueta', axis=1)\n",
    "y = data['etiqueta']\n",
    "\n",
    "# Paso 3: Dividir los datos en conjuntos de entrenamiento y prueba\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Paso 4: Crear un pipeline con modelos para comparar y seleccionar\n",
    "models = [('Linear Regression', LinearRegression()),\n",
    "          ('Support Vector Machine', SVR()),\n",
    "          ('Random Forest', RandomForestRegressor())]\n",
    "\n",
    "results = []\n",
    "names = []\n",
    "\n",
    "for name, model in models:\n",
    "    pipeline = Pipeline([('scaler', StandardScaler()), (name, model)])\n",
    "    \n",
    "    # Ajuste de hiperparámetros utilizando GridSearchCV\n",
    "    if name == 'Linear Regression':\n",
    "        param_grid = {'Linear Regression__fit_intercept': [True, False]}\n",
    "    elif name == 'Support Vector Machine':\n",
    "        param_grid = {'Support Vector Machine__C': [0.1, 1, 10],\n",
    "                      'Support Vector Machine__kernel': ['linear', 'rbf']}\n",
    "    elif name == 'Random Forest':\n",
    "        param_grid = {'Random Forest__n_estimators': [50, 100, 200],\n",
    "                      'Random Forest__max_depth': [None, 10, 20]}\n",
    "    \n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=3)\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    best_model = grid_search.best_estimator_\n",
    "    \n",
    "    # Entrenar el modelo por minibatch (opcional)\n",
    "    if hasattr(model, 'partial_fit'):\n",
    "        batch_size = 32\n",
    "        for i in range(0, len(X_train), batch_size):\n",
    "            X_batch = X_train[i:i + batch_size]\n",
    "            y_batch = y_train[i:i + batch_size]\n",
    "            best_model.partial_fit(X_batch, y_batch)\n",
    "    else:\n",
    "        best_model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluar el modelo en el conjunto de prueba\n",
    "    y_pred = best_model.predict(X_test)\n",
    "    \n",
    "    # Métricas de rendimiento\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results.append((mse, r2))\n",
    "    names.append(name)\n",
    "    \n",
    "    print(f\"Modelo: {name}\")\n",
    "    print(\"Mejores hiperparámetros:\", grid_search.best_params_)\n",
    "    print(\"Mean Squared Error:\", mse)\n",
    "    print(\"R-squared:\", r2)\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "# Paso 5: Comparar y seleccionar el mejor modelo\n",
    "best_model_name = names[np.argmin([mse for mse, _ in results])]\n",
    "best_model = [model[1] for model in models if model[0] == best_model_name][0]\n",
    "\n",
    "print(f\"Mejor Modelo: {best_model_name}\")\n",
    "\n",
    "# Paso 6: Entrenar el mejor modelo en el conjunto de entrenamiento completo\n",
    "best_pipeline = Pipeline([('scaler', StandardScaler()), (best_model_name, best_model)])\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Paso 7: Evaluar el mejor modelo en el conjunto de prueba\n",
    "y_pred = best_pipeline.predict(X_test)\n",
    "\n",
    "mse_best = mean_squared_error(y_test, y_pred)\n",
    "r2_best = r2_score(y_test, y_pred)\n",
    "\n",
    "print(f\"Mejor Modelo: {best_model_name}\")\n",
    "print(\"Mean Squared Error:\", mse_best)\n",
    "print(\"R-squared:\", r2_best)\n",
    "\n",
    "# Paso 8: Gráficas para visualizar el rendimiento del mejor modelo (opcional)\n",
    "# Por ejemplo, una gráfica de dispersión entre las etiquetas reales y las predicciones\n",
    "plt.scatter(y_test, y_pred)\n",
    "plt.xlabel('Etiquetas reales')\n",
    "plt.ylabel('Predicciones')\n",
    "plt.title('Gráfica de dispersión')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
